{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Load, save and manipulate databases\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from importlib import reload\n",
    "\n",
    "import src.globals as g\n",
    "\n",
    "from src.anomalies import NoiseMachine\n",
    "from src.spectrum import SpectrumDecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(g.path_tracks_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PrePreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels=\"Unnamed: 0\", axis=1)\n",
    "manipulators = df['manipulator'].apply(lambda x : \"1\" if \"1\" in x else (\"2\" if \"2\" in x else \"?\"))\n",
    "df.insert(0, 'robotid', df['id'] + \"-\" + manipulators)\n",
    "df.insert(0, 'seqid', df['robotid'] + \"|\" + df['date'] + \"|\" + df['time'])\n",
    "df = df.drop(labels=[\"id\", \"manipulator\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small fixes, sort it based on seqid&timeindex. Remove known faulty sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['seqid', 'timeindex'])\n",
    "df = df[df['seqid'] != '6640-103454-1|2021-06-03|06:16:54']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the input database such that all series start at a timeindex of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df.groupby('seqid').agg(timeindex_min=('timeindex', 'min')).reset_index()\n",
    "df_tmp = df.merge(df_tmp, on='seqid')\n",
    "df_tmp['timeindex'] = df_tmp['timeindex'] - df_tmp['timeindex_min']\n",
    "df_tmp = df_tmp.drop(labels=[\"timeindex_min\"], axis=1)\n",
    "df = df_tmp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the tracks input database such that it always have a negative slope, i.e. from positive mean torqueactual to negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark every series that has this problem.\n",
    "multdf = df.groupby(['seqid']).apply(lambda v: -1 if linregress(v['timeindex'], v['torqueactual'])[0] > 0 else 1).reset_index(name=\"multiplier\")\n",
    "multdf2 = df.groupby(['seqid']).apply(lambda v: max(v['motorposition']) + min(v['motorposition']) if linregress(v['timeindex'], v['torqueactual'])[0] > 0 else 0).reset_index(name=\"posfix\")\n",
    "\n",
    "# Merge this database with the original one.\n",
    "df = df.merge(multdf, on=['seqid'])\n",
    "df = df.merge(multdf2, on=['seqid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now multiply it, such that we always start positive and end negative.\n",
    "df['torqueactual'] = df['torqueactual'] * df['multiplier']\n",
    "df['speedsetpoint'] = df['speedsetpoint'] * df['multiplier']\n",
    "df['motorposition'] = abs(df['posfix'] - df['motorposition'])\n",
    "\n",
    "\n",
    "#drop multiplier column, since it has done its job\n",
    "df = df.drop(labels=[\"multiplier\", \"posfix\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding timebins & Interpolate on the timebins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using bins instead of more timeindices \n",
    "df = df.sort_values(by=['seqid', 'timeindex'])\n",
    "timeindex_dt = np.median(df['timeindex'].diff().fillna(0))\n",
    "df['timeindex_bin'] = np.round(df['timeindex'] / timeindex_dt).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before interpolation, we need to remove motorposition outliers\n",
    "df['motorposition_diff'] = df['motorposition'].diff()\n",
    "df.loc[df['motorposition_diff'].abs() > 100, 'motorposition'] = np.nan\n",
    "df = df.drop(labels=[\"motorposition_diff\"], axis=1)\n",
    "\n",
    "#Interpolation\n",
    "def fill_and_interpolate(group):\n",
    "    columns_intrp = ['timeindex', 'motorposition', 'speedsetpoint', 'torquefeedforward', 'torqueactual']\n",
    "    columns_fill = ['seqid', 'robotid', 'date', 'time']\n",
    "\n",
    "    # Add rows that do not have values\n",
    "    full_range = pd.DataFrame({'timeindex_bin': range(0, group['timeindex_bin'].max() + 1)})\n",
    "    group = pd.merge(full_range, group, how='left', on=['timeindex_bin'])\n",
    "    group.sort_values('timeindex_bin')\n",
    "\n",
    "    # Fill & Interpolate\n",
    "    group[columns_intrp] = group[columns_intrp].interpolate(method='linear')\n",
    "    group[columns_fill] = group[columns_fill].ffill().bfill()\n",
    "\n",
    "    return group\n",
    "\n",
    "df = df.groupby('seqid').apply(fill_and_interpolate).reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the sequences that have timeindices that are unrealistic.\n",
    "df_tmp = df.groupby('seqid').agg(robotid=('robotid', 'first'), date=('date', 'first'), time=('time', 'first'), timeindex=('timeindex', 'max')).reset_index()\n",
    "df_tmp2 = df_tmp.groupby('robotid').agg(timeindex_mean=('timeindex', 'mean'), timeindex_std=('timeindex', 'std')).reset_index()\n",
    "df_tmp = df_tmp.merge(df_tmp2, on='robotid')\n",
    "df_tmp['zscore'] = (df_tmp['timeindex'] - df_tmp['timeindex_mean']) / df_tmp['timeindex_std']\n",
    "df_tmp = df_tmp[(abs(df_tmp['zscore']) >= 1.5) & (df_tmp['timeindex_std'] >= 1)].reset_index()\n",
    "\n",
    "#Now remove these sequences.\n",
    "df = df[~df['seqid'].isin(df_tmp['seqid'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Processed\n",
    "df.to_parquet(g.path_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(g.path_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror(group):\n",
    "    mindex = group.index.min()\n",
    "    midpoint = (len(df) // 2) + mindex\n",
    "\n",
    "    # Mirror the column around the midpoint\n",
    "    group.loc[midpoint:, 'torqueactual'] = group.loc[midpoint:, 'torqueactual'].iloc[::-1].values\n",
    "    group['torqueactual'] = group['torqueactual'] * -1\n",
    "\n",
    "    return group\n",
    "\n",
    "df_mirror           = df.copy().groupby('seqid').apply(mirror).reset_index(drop=True)\n",
    "df_mirror['seqid']  = df_mirror['seqid'] + '|mirrored'\n",
    "df['seqid']         = df['seqid'] + \"|normal\"\n",
    "\n",
    "df_up = pd.concat([df, df_mirror]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving up         \n",
    "df_up.to_parquet(g.path_tracks_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthethic Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up = pd.read_parquet(g.path_tracks_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100000  # Adjust this size based on your memory limits\n",
    "seqid_groups = df_up.groupby('seqid')\n",
    "\n",
    "results = []\n",
    "for _, group in seqid_groups:\n",
    "    anomalies = NoiseMachine.generate_anomalies(group)\n",
    "    results.append(anomalies)\n",
    "    if len(results) % chunk_size == 0:\n",
    "        df_syn = pd.concat(results, ignore_index=True)\n",
    "        # Optionally save or process each chunk if you can't keep all in memory\n",
    "        results = []  # Clear processed results to free memory\n",
    "\n",
    "# Concatenate any remaining results\n",
    "if results:\n",
    "    df_syn = pd.concat(results, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['6640-101753-1|2021-03-01|10:37:33|mirrored|anom_gaussian',\n",
       "       '6640-101753-1|2021-03-01|10:37:33|mirrored|anom_point',\n",
       "       '6640-101753-1|2021-03-01|10:37:33|mirrored|anom_sinus', ...,\n",
       "       '7600-100498-1|2022-03-02|04:58:34|normal|anom_gaussian',\n",
       "       '7600-100498-1|2022-03-02|04:58:34|normal|anom_point',\n",
       "       '7600-100498-1|2022-03-02|04:58:34|normal|anom_sinus'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_syn['seqid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16254"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_syn['seqid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() got an unexpected keyword argument 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_syn), chunk_size):\n\u001b[0;32m      3\u001b[0m     df_chunk \u001b[38;5;241m=\u001b[39m df_syn\u001b[38;5;241m.\u001b[39miloc[i:i \u001b[38;5;241m+\u001b[39m chunk_size]\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mdf_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_tracks_syn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2804\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2885\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2890\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    409\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m--> 411\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parquet.py:189\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_to_dataset(\n\u001b[0;32m    181\u001b[0m             table,\n\u001b[0;32m    182\u001b[0m             path_or_handle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    186\u001b[0m         )\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyarrow\\parquet\\core.py:1869\u001b[0m, in \u001b[0;36mwrite_table\u001b[1;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, **kwargs)\u001b[0m\n\u001b[0;32m   1867\u001b[0m use_int96 \u001b[38;5;241m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1869\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1874\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_page_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1880\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_int96\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1891\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1892\u001b[0m \u001b[43m            \u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorting_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1893\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m   1894\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(table, row_group_size\u001b[38;5;241m=\u001b[39mrow_group_size)\n\u001b[0;32m   1895\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyarrow\\parquet\\core.py:1002\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[1;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, **options)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_collector \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_collector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1001\u001b[0m engine_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1002\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m \u001b[43m_parquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43msink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_engine_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorting_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_open \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyarrow\\_parquet.pyx:2101\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __cinit__() got an unexpected keyword argument 'append'"
     ]
    }
   ],
   "source": [
    "chunk_size = 10000  # Adjust based on your system memory\n",
    "for i in range(0, len(df_syn), chunk_size):\n",
    "    df_chunk = df_syn.iloc[i:i + chunk_size]\n",
    "    df_chunk.to_parquet(g.path_tracks_syn, engine='pyarrow', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Spectrum Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SpectrumDecomposition(col_name=\"torqueactual\", n_freq=25)\n",
    "df_decomposed = sd.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df[g.tracks_result_columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
